{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['tagger', 'parser', 'ner', 'lemmatizer', 'textcat', 'custom'])\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(doc):\n",
    "    all_tokens = []\n",
    "    for token in doc:\n",
    "        all_tokens.append(token.text)\n",
    "        if len(token.whitespace_):\n",
    "            all_tokens.append(token.whitespace_)\n",
    "    return all_tokens\n",
    "\n",
    "def make_changes(nlp, source_sentence, target_sentences = [], min_count = 2, debug=False):\n",
    "\n",
    "    source_tokens = get_tokens(nlp(str(source_sentence)))\n",
    "    \n",
    "    target_docs_tokens = [get_tokens(nlp(str(sent))) for sent in target_sentences]\n",
    "    all_actions = []\n",
    "\n",
    "        \n",
    "    for i in range(len(target_sentences)):\n",
    "\n",
    "        target_tokens = target_docs_tokens[i]\n",
    "        \n",
    "        matcher = SequenceMatcher(None, source_tokens, target_tokens)\n",
    "        \n",
    "        raw_diffs = list(matcher.get_opcodes())\n",
    "        \n",
    "        \n",
    "        for diff in raw_diffs:\n",
    "            if diff[0] == 'replace':\n",
    "                #\"source_start_token\", \"source_end_token\", \"target_part\"\n",
    "                all_actions.append(\n",
    "                    ('replace', diff[1], diff[2], \"\".join(target_tokens[diff[3] : diff[4]])) \n",
    "                )\n",
    "            if diff[0] == 'delete':\n",
    "                #\"source_start_token\", \"source_end_token\"\n",
    "                all_actions.append(\n",
    "                    ('delete', diff[1], diff[2])\n",
    "                )\n",
    "            if diff[0] == 'insert':\n",
    "                #\"source_start_token\", \"target_part\"\n",
    "                all_actions.append(\n",
    "                    ('insert', diff[1], \"\".join(target_tokens[diff[3] : diff[4]]))\n",
    "                )\n",
    "     \n",
    "    \n",
    "    good_actions = [k for k,v in Counter(all_actions).items() if v >= min_count]\n",
    "    good_actions.sort(key=lambda x: x[1]) #sort by second field - start token\n",
    "    \n",
    "    if debug:\n",
    "        print(\"All actions\", all_actions)\n",
    "        print(\"Good actions\", good_actions)\n",
    "    \n",
    "    if len(good_actions) > 0:\n",
    "        \n",
    "        final_text = \"\"\n",
    "        current_start = 0\n",
    "        previous_end =  0\n",
    "        \n",
    "        for action in good_actions:\n",
    "            current_start = action[1]\n",
    "            final_text += \"\".join(source_tokens[previous_end : current_start])\n",
    "            if action[0] == 'replace':\n",
    "                final_text += action[3]\n",
    "                previous_end = action[2]\n",
    "            if action[0] == 'delete':\n",
    "                previous_end = action[2]\n",
    "            if action[0] == 'insert':\n",
    "                final_text += action[2]\n",
    "                previous_end = action[1]\n",
    "        \n",
    "        final_text += \"\".join(source_tokens[previous_end :])\n",
    "        return final_text\n",
    "            \n",
    "    else:\n",
    "        return ''.join(source_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "def read_lines(fn):\n",
    "    if not os.path.exists(fn):\n",
    "        return []\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    lines = text.split(\"\\n\")\n",
    "    if lines[-1] == '':\n",
    "        return lines[:-1]\n",
    "    else:\n",
    "        return lines\n",
    "\n",
    "def write_lines(fn, lines, mode='w'):\n",
    "    text_to_write = \"\\n\".join(list(lines)) \n",
    "    with open(fn, encoding='utf-8', mode=mode) as f:\n",
    "        f.write(text_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../data_parallel/wi+locness/dev_src\"\n",
    "source_sentences = read_lines(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../predicts/\"\n",
    "#sorted(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(os.listdir(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts = [\n",
    "#     'Exp_037_roberta_large_st3_epoch_1.txt',\n",
    "#     'Exp_038_deberta_large_st3_epoch_0.txt',\n",
    "#     'Exp_039_xlnet_large_st3_epoch_2.txt'\n",
    "# ]\n",
    "\n",
    "# predicts = [\n",
    "#     'Exp_043_roberta_large_ac_0.05_mep_0.6.txt',\n",
    "#     'Exp_044_deberta_large_ac_0.4_mep_0.55.txt',\n",
    "#     'Exp_045_xlnet_large_ac_0.3_mep_0.6.txt'\n",
    "# ]\n",
    "\n",
    "predicts = [\n",
    "    'Exp_043_roberta_large_ac_0.2_mep_0.6.txt',\n",
    "    'Exp_044_deberta_large_ac_0.2_mep_0.6.txt',\n",
    "    'Exp_045_xlnet_large_ac_0.3_mep_0.55.txt'\n",
    "]\n",
    "\n",
    "pred_texts = []\n",
    "for pred_path in predicts:\n",
    "    path = folder_path+pred_path\n",
    "    pred_texts.append(read_lines(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62fcdbf8dcf4a70b8b6b7ae42ce1e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4384.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_texts = np.array(pred_texts)\n",
    "sent_after_merge = []\n",
    "for i in tqdm(range(len(source_sentences))):\n",
    "    source_sentence = source_sentences[i]\n",
    "    target_sentences = pred_texts[:,i]\n",
    "    new_sentence = make_changes(nlp, source_sentence, target_sentences = target_sentences, min_count = 2, debug=False)\n",
    "    sent_after_merge.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_texts = [str(t) for t in sent_after_merge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lines(folder_path+\"pred_roberta_deberta_xlnet_after_tweeks_ac_023_mep_0655.txt\", pred_texts, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errant_parallel -orig data_parallel/wi+locness/dev_src -cor predicts/pred_roberta_deberta_xlnet_after_tweeks_ac_023_mep_0655.txt -out evaluation/pred_roberta_deberta_xlnet_after_tweeks_ac_023_mep_0655.m2\n",
      "\n",
      "errant_compare -hyp evaluation/pred_roberta_deberta_xlnet_after_tweeks_ac_023_mep_0655.m2 -ref data_m2/wi+locness/ABCN.dev.gold.bea19.m2\n"
     ]
    }
   ],
   "source": [
    "output_part = \"pred_roberta_deberta_xlnet_after_tweeks_ac_023_mep_0655\"\n",
    "print(\"errant_parallel -orig data_parallel/wi+locness/dev_src -cor predicts/\"+output_part+\".txt -out evaluation/\"+output_part+\".m2\")\n",
    "print()\n",
    "print(\"errant_compare -hyp evaluation/\"+output_part+\".m2 -ref data_m2/wi+locness/ABCN.dev.gold.bea19.m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge(predicts):\n",
    "    pred_texts = []\n",
    "    for pred_path in predicts[:3]:\n",
    "        path = folder_path+pred_path\n",
    "        pred_texts.append(read_lines(path))\n",
    "    pred_texts = np.array(pred_texts)\n",
    "    sent_after_merge = []\n",
    "    for i in tqdm(range(len(source_sentences))):\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentences = pred_texts[:,i]\n",
    "        new_sentence = make_changes(nlp, source_sentence, target_sentences = target_sentences, min_count = 2, debug=False)\n",
    "        sent_after_merge.append(new_sentence)\n",
    "    pred_texts = [str(t) for t in sent_after_merge]\n",
    "    write_lines(folder_path+predicts[3], pred_texts, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(combos):\n",
    "    for combo in combos:\n",
    "        generate_merge(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(all_comb, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = Pool(10)\n",
    "# result_map = pool.map(process_batch, chunks)\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try find best combination after tweeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = os.listdir(\"../predicts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_preds = sorted([pred for pred in all_preds if \"Exp_045\" in pred])\n",
    "roberta_preds = sorted([pred for pred in all_preds if \"Exp_043\" in pred])\n",
    "deberta_preds = sorted([pred for pred in all_preds if \"Exp_044\" in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_confidences = [round(0.05*i,3) for i in range(3,11)]\n",
    "additional_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_error_probabilities = [round(0.05*i,3) for i in range(6,16)]\n",
    "min_error_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(additional_confidences)*len(min_error_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comb = []\n",
    "for ac in additional_confidences:\n",
    "    for mep in min_error_probabilities:\n",
    "        s_ac = str(ac)\n",
    "        s_mep = str(mep)\n",
    "        comb =  [\n",
    "            'Exp_043_roberta_large_ac_'+s_ac+'_mep_'+s_mep+'.txt',\n",
    "            'Exp_044_deberta_large_ac_'+s_ac+'_mep_'+s_mep+'.txt',\n",
    "            'Exp_045_xlnet_large_ac_'+s_ac+'_mep_'+s_mep+'.txt',\n",
    "            'Exp_merge_large_ac_'+s_ac+'_mep_'+s_mep+'.txt'\n",
    "        ]\n",
    "        all_comb.append(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
